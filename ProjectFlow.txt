-------------------------Setting up project structure---------------------------

1. Create repo, clone it in local
2. Run "uv init" to initialize the virtual environment.
3) Run "uv add cookiecutter scikit-learn dagshub mlflow" to add the dependencies to the virtual environment. There might be many more. 
   Add whenever you face issue
4) Run "uv pip instsll -e ." to add the local packages to the environment.
5. cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science
6. Rename src.models -> src.model
7. git add - commit - push

-------------------------Setup MLFlow on Dagshub---------------------------
8. Go to: https://dagshub.com/dashboard
9. Create > New Repo > Connect a repo > (Github) Connect > Select your repo > Connect
10. Copy experiment tracking url and code snippet. (Also try: Go To MLFlow UI)

https://dagshub.com/pradkris07/Diabetes-Health-Indicators.mlflow

import dagshub
dagshub.init(repo_owner='pradkris07', repo_name='Diabetes-Health-Indicators', mlflow=True)

import mlflow
with mlflow.start_run():
  mlflow.log_param('parameter name', 'value')
  mlflow.log_metric('metric name', 1)
  
11) uv add dagshub & mlflow
12) Run the notebook to get an understanding of the datafile. Also try different ML methods to get the optimum method for prediction.
13) git add - commit - push

-------------------------------------------------------------------------------

14) dvc init 
15) create a local folder as "local_s3" (temporary work)
16) on terminal - "dvc remote add -d mylocal local_s3"
17) Add code to below files/folders inside src dir:
    - logger
	  - to help in debugging
    - data_ingestion.py
	  - program to extract the dataset from Github dataset repo to local machine, extract a few sample records for using in front end, split the records
	    into test and train files.
    - data_validation.py
	  - program to validate the columns of the files, its column type and number of columns in the datasets.
    - data_transformation.py
	  - program to apply scaling on the data, handling outliers, balancing the dataset using SMOTEENN.
    - model_trainer.py
	  - program to train using different models with various hyper-parameters using RandomizedSearchCV and records its performance metrics. This will also 
	    update a final.yaml file with the best performance hyperparameter for each model.
    - register_model.py
	  - program to register the model in the MLFlow and the metrics in DVC.
	- model.yaml
	  - Parameters related to the different models trained by the program.
	- dvc.yaml
	  - pipeline used for running the program.
18) DVC pipeline is ready to run - dvc repro
19) Once do - dvc status
20) git add - commit - push

21) Create new dir - flask_app | Inside that, add rest of the files and dir
22) uv add flask and run the app.
23) You can enter any value in the form to check the prediction.
24) You will also have a training link if we need to update the model with new data.
	